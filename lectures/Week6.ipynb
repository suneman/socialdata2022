{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "\n",
    "Phew. Is it week 6 already? \n",
    "\n",
    "Last week we had an intro to machine learning and regression and this week we continue with some more ML but focusing on classification instead. There are lots of courses on machine learning at DTU. And across many research areas, people use ML for all kinds of things. So there's a good chance you're already familiar with what's going to happen today. \n",
    "\n",
    "In the following, we continue introducing fundamentals of ML, decision trees and start with some prediction tasks on crime data. You might ask, why are we doing this? Well, a couple of reasons:\n",
    "\n",
    "1. It ties nicely with how we started this course: do you remember all we learnt about predictive policing in Week 1? So, today it is our turn to make predictions and see how well we can do with the data we have been exploring.\n",
    "\n",
    "2. Visualization **AND** machine learning is a powerful combination. A combination that is pretty rare. \n",
    "  - Usually it's the case that people are either good at machine learning or data viz, but not both. \n",
    "  - So what we will be able to do in this class is an unusual combo: We can use ML to understand data and then visualize the outputs of the machine-learning.\n",
    "    \n",
    "The plan for today is as follows:\n",
    "\n",
    "1. In part 1, we go more in depth on fundamentals of machine learning;\n",
    "2. In part 2, we get an introduction to Decision Trees;\n",
    "3. In part 3, we put everything together to predict criminal recidivism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fundamentals of machine learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with a couple of lectures from Ole Winter about model selection and feature extraction. These connect nicely with what you should have already read in DSFS Chaper 11. If you did not read the chater yet, it is time for you to do it. \n",
    "\n",
    "Find it on DTU Learn under 'Course content' $\\rightarrow$ 'Content' $\\rightarrow$ 'Lecture 6 reading' \n",
    "\n",
    "**Model selection**\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/MHhlAtw3Ces/0.jpg)](https://www.youtube.com/watch?v=MHhlAtw3Ces)\n",
    "\n",
    "**Feature extraction and selection**\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/RZmitKn220Q/0.jpg)](https://www.youtube.com/watch?v=RZmitKn220Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 1*: A few questions about machine learning to see whether you've read the text and watched the videos. \n",
    ">\n",
    "> * What do we mean by a 'feature' in a machine learning model?\n",
    "> * What is the main problem with overfitting?\n",
    "> * Explain the connection between the bias-variance trade-off and overfitting/underfitting.\n",
    "> * The `Luke is for leukemia` on page 145 in the reading is a great example of why accuracy is not a good measure in very unbalanced problems. Try to come up with a similar example based on a different type of data (either one you are interested in or one related to the SF crime dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Decision Tree Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to decision trees. This is a fantastically useful supervised machine-learning method, that we use all the time in research. To get started on the decision trees, we asked you to read DSFS, chapter 17 (if you didn't read it you can find it in DTU Learn). \n",
    "\n",
    "And our little session on decision trees wouldn't be complete without hearing from Ole about these things. \n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/LAA_CnkAEx8/0.jpg)](https://www.youtube.com/watch?v=LAA_CnkAEx8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 2:* Just a few questions to make sure you've read the text (DSFS chapter 17) and/or watched the video.\n",
    "> \n",
    "> * There are two main kinds of decision trees depending on the type of output (numeric vs. categorical). What are they?\n",
    "> * Explain in your own words: Why is entropy useful when deciding where to split the data?\n",
    "> * Why are trees prone to overfitting?\n",
    "> * Explain (in your own words) how random forests help prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following I added some additional material for you to explore decision trees through some fantastic *visual* introductions. \n",
    "\n",
    "*Decision Trees 1*: The visual introduction to decision trees on this webpage is AMAZING. Take a look to get an intuitive feel for how trees work. Do not miss this one, it's a treat! http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n",
    "*Decision Trees 2*: the second part of the visual introduction is about the topic of model selection, and bias/variance tradeoffs that we looked into earlier during this lesson. But once again, here those topics are visualized in a fantastic and inspiring way, that will make it stick in your brain better. So check it out http://www.r2d3.us/visual-intro-to-machine-learning-part-2/\n",
    "\n",
    "\n",
    "\n",
    "*Decision tree tutorials*: And of course the best way to learn how to get this stuff rolling in practice, is to work through a tutorial or two. We recommend the ones below:\n",
    "  * https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "  * https://towardsdatascience.com/random-forest-in-python-24d0893d51c0 (this one also has good considerations regarding the one-hot encodings)\n",
    "  \n",
    "(But there are many other good ones out there.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predicting criminal recidivism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to put everything together and use the models we have read about for prediction. Today, we are still going to focus on crimes, but with a different dataset. \n",
    "\n",
    "The dataset is related to an algorithm used by judges and parole officers for scoring criminal defendantâ€™s likelihood of reoffending (recidivism). It consists of information about defendants and variables used to measure recidivism. \n",
    "\n",
    "I'll provide you with more information about this data and its source next week. But, for now I don't want to give you more spoilers (you'll know why next week ðŸ˜‡), so let's get started. In the next exercises, we will try to **loosely** recreate the algorithm to predict whether a person is going to re-commit a crime in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 3.1:* Getting the data ready. Before getting to predictions, we need to get the data, select the features, and define the target. Follow these steps for success:\n",
    ">\n",
    "> * Download the dataset from [GitHub](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/recidivism_dataset_sub.csv) and load it in a `pandas` dataframe.\n",
    "> * Select the variables of interest. Here, a description of which one and their meaning:\n",
    ">    1. `age`: age (in years) of the person,;\n",
    ">    2. `sex`: either \"Female\" or \"Male\";\n",
    ">    3. `race`: a variable encoding the race of the person;\n",
    ">    4. `juv_fel_count`: the number of previous juvenile felonies;\n",
    ">    5. `juv_misd_count`: the number of previous juvenile misdemeanors;\n",
    ">    6. `juv_other_count`: the number of prior juvenile convictions that are not considered either felonies or misdemeanors;\n",
    ">    7. `priors_count`: the number of prior crimes committed;\n",
    ">    8. `is_recid`: if the defendent has recommit a crime;\n",
    ">    9. `days_b_screening_arrest`: Days between the arrest and COMPAS screening.\n",
    ">    9. `c_charge_degree`: Degree of the crime. It is either M (Misdemeanor), F (Felony), or O (not causing jail)\n",
    ">\n",
    "> * Finally, we need a target:\n",
    ">    * `two_year_recid` is what we want to predict. Its current values are $\\in\\left[0,1\\right]$, where $0$ means the defendant did not recommit a crime within two years, and $1$ means the defendant recommitted a crime within two years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we now have the data, but we still need a bit of **preprocessing** before we can get to the actual prediction.\n",
    "\n",
    "At the beginning, I wanted you to embed everything into a unique pipeline. I later found that it sometimes have issues (throw errors, takes long time when cross-validating, etc.). Thus, I have excluded this step from today's class. However, if you want to know more about pipelines, here, a nice optional tutorial for you:\n",
    "\n",
    "* https://towardsdatascience.com/step-by-step-tutorial-of-sci-kit-learn-pipeline-62402d5629b6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 3.2:* Data preprocessing and label encoding. \n",
    ">\n",
    "> * To preprocess the data follow these steps:\n",
    ">    * filter out records where the `is_recid` feature is not known (i.e. where it is equal to -1); \n",
    ">    * only keep records that cause jail time;\n",
    ">    * only keep records that have between $-30$ and $30$ days between the arrest and screening.\n",
    ">    * Finally, drop `is_recid`, `c_charge_degree`, `days_b_screening_arrest` for the upcoming analysis.\n",
    "> * Before we move on, let's explore the data with a few visualizations. Use the variable `two_year_recid` and create a plot with the following subplots: \n",
    ">     * A bar plot with the number of recommitted and non-recommitted crimes. What is the fraction of recommitted crimes over the total number of records? Is it balanced?\n",
    ">     * A bar plot with the fraction of recidivism per `sex`. What do you observe? \n",
    ">     * A bar plot with the fraction of recidivism per `race`. What do you observe?\n",
    ">     * A bar plot with the fraction of recidivism per `age` group (group ages as <20, 20-30, 30-40, etc.). What do you observe?\n",
    "> * Some features we are working with are categorical, so we need to deal with them by using encoders. There are many different types, but we will focus on the `OneHotEncoder` and the `LabelEncoder`:\n",
    ">    * Describe what these encoder do and choose one. Which one did you choose? Why?\n",
    ">    * What variables need to be transformed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost there! It is now time to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 3.3:* Build a Decision Tree or a Random Forest. Now we are going to build a Decision Tree (or a Random Forest) classifier that takes as input the features defined above and predicts if a person is going to recommit the crime within two years.\n",
    "> * Split the data in Train/Test sets. You can do this with `train_test_split` in `sklearn`, I used a 70/30 split, but you are free to try different ones. \n",
    ">     * **Note:** create a balanced dataset, that is, **grab an equal number of examples** from each target value.\n",
    ">    * Fit a model to your Train set. A good option is the  `DecisionTreeClassifier` (or even better a [Random Forest](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html), here is [another tutorial for Random Forests](https://towardsdatascience.com/random-forest-in-python-24d0893d51c0)).\n",
    "> * Evaluate the performance of model on the test set (look at Accuracy, Precision, and Recall). What are your thoughts on these metrics? Is accuracy a good measure?\n",
    ">    * **hint:** Since you have created a balanced dataset, the baseline performance (random guess) is 50%. \n",
    "> * Are your results tied to the specific training data/hyperparameter set you used? Try to perform a `RandomizedSearchCV` and recompute the performance metric above with the hyperparameters found. [Here](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) a nice tutorial for you! And here one on [cross-validation](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f) for those of you who crave for more.\n",
    "> * Visualize the tree. There are different options to do so. The easiest one is to use `plot_tree`, but there are other [options](https://mljar.com/blog/visualize-decision-tree/). If you chose Random Forest, you can visualize a tree as well by extracting a single tree with `model.estimators_[n]` (n is the index of the estimator you want to select).\n",
    "> * Visualize the Feature Importance. What do you observe?\n",
    "> * **(Optional)** If you find yourself with extra time, come back to this exercise and tweak the encoder, model, and variables you use to see if you can improve the performance of the tree. **Note**: It's not 100% given that adding variables will improve your predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you go, please, have a look at the following two activities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1)\n",
    "\n",
    "<mark> Take a minute (it is really one minute) to fill this [form](https://forms.gle/9RwhFc96na4E2Fmg7). It is really important for me to continue improving and give you better feedbacks. </mark>\n",
    "\n",
    "---\n",
    "2)\n",
    "\n",
    "<mark> Some of you consider this course too easy. So, it's time to spice things up: once you have the best model you could find, go to DTU Learn and submit your code together with your final accuracy/precision/recall scores under DTU-Learn $\\rightarrow$ Assignments. I'll make a Leaderboard and we'll see who's gonna win ðŸ¥‡!!</mark>\n",
    "\n",
    "**Note 1:** Even if it is in the form of an assignment on DTU Learn it is **not** going to be evaluated. So, take it really as an opportunity to play around with your model and see how well you can do.\n",
    "\n",
    "**Note 2:** You have time until **Thursday at 23.59** to submit your model/performance score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
